{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code version 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50063464.0\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (input)\n",
    "y = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (output)\n",
    "w1 = tf.placeholder(tf.float32, shape = (D, H))                              # 1000 x 100 placeholder   (weights)\n",
    "w2 = tf.placeholder(tf.float32, shape = (H, D))                              # 100 x 1000 placeholder   (weights)\n",
    "\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)                                          # Temporary variable contains the result of multiplyint x and w1\n",
    "y_pred = tf.matmul(h, w2)                                                    # The predicted values of the output\n",
    "diff = y_pred - y                                                            # Calculating the difference between the predicted and the real values of the output\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis = 1))                      # Calculating the loss function by using the mean square error method\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])                              # Calculate the loss of gradinet with respect to w1 and w2\n",
    "\n",
    "with tf.Session() as sess:                                                   # Opening a session\n",
    "    values = {x: np.random.randn(N, D),                                      # Assigning a random values for the placeholders\n",
    "              w1: np.random.randn(D, H),\n",
    "              w2: np.random.randn(H, D),\n",
    "              y: np.random.randn(N, D)}\n",
    "    out = sess.run([loss, grad_w1, grad_w2], feed_dict = values)            # Running the computational graph\n",
    "    loss_val, grad_w1_val, grad_w2_val = out                                # Assigning the output to variables\n",
    "\n",
    "print(loss_val)                                                             # Printing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code version 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50311012.0\n",
      "17496160.0\n",
      "9238482.0\n",
      "5465027.0\n",
      "3460670.5\n",
      "2294339.5\n",
      "1574057.5\n",
      "1108183.8\n",
      "795969.6\n",
      "580745.4\n",
      "428945.5\n",
      "320130.84\n",
      "240870.0\n",
      "182514.86\n",
      "139210.42\n",
      "106787.39\n",
      "82355.016\n",
      "63812.168\n",
      "49668.273\n",
      "38813.83\n",
      "30483.523\n",
      "24050.203\n",
      "19067.902\n",
      "15185.591\n",
      "12161.664\n",
      "9797.135\n",
      "7943.6104\n",
      "6487.9326\n",
      "5341.41\n",
      "4437.5566\n",
      "3723.481\n",
      "3158.122\n",
      "2710.4688\n",
      "2355.376\n",
      "2073.354\n",
      "1849.0929\n",
      "1670.6313\n",
      "1528.4365\n",
      "1415.0868\n",
      "1324.685\n",
      "1252.5925\n",
      "1195.0593\n",
      "1149.1765\n",
      "1112.5808\n",
      "1083.3813\n",
      "1060.1294\n",
      "1041.6132\n",
      "1026.8856\n",
      "1015.18634\n",
      "1005.9175\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (input)\n",
    "y = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (output)\n",
    "w1 = tf.placeholder(tf.float32, shape = (D, H))                              # 1000 x 100 placeholder   (weights)\n",
    "w2 = tf.placeholder(tf.float32, shape = (H, D))                              # 100 x 1000 placeholder   (weights)\n",
    "\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)                                          # Temporary variable contains the result of multiplyint x and w1\n",
    "y_pred = tf.matmul(h, w2)                                                    # The predicted values of the output\n",
    "diff = y_pred - y                                                            # Calculating the difference between the predicted and the real values of the output\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis = 1))                      # Calculating the loss function by using the mean square error method\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])                              # Calculate the loss of gradinet with respect to w1 and w2\n",
    "\n",
    "with tf.Session() as sess:                                                   # Opening a session\n",
    "    values = {x: np.random.randn(N, D),                                      # Assigning a random values for the placeholders\n",
    "              w1: np.random.randn(D, H),\n",
    "              w2: np.random.randn(H, D),\n",
    "              y: np.random.randn(N, D)}\n",
    "    learning_rate = 1e-5                                                    # Assigne the learning rate\n",
    "    for t in range(50):                                                     # traning loop with 50 iterations\n",
    "        out = sess.run([loss, grad_w1, grad_w2], feed_dict = values)        # Running the computational graph\n",
    "        loss_val, grad_w1_val, grad_w2_val = out                            # Assigning the output to variables\n",
    "        values[w1] -= learning_rate*grad_w1_val                             # Use gradients to update weights\n",
    "        values[w2] -= learning_rate*grad_w2_val                             # Use gradients to update weights\n",
    "        print(loss_val)                                                     # Printing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code in the previous cell has a problem of copying weights between CPU and GPU each step, so we will do:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code version 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n",
      "53133656.0\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (input)\n",
    "y = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (output)\n",
    "w1 = tf.Variable(tf.random_normal((D, H)))                                   # 1000 x 100 variable   (weights)\n",
    "w2 = tf.Variable(tf.random_normal((H, D)))                                   # 100 x 1000 variable   (weights)\n",
    "\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)                                          #**code version 1** Temporary variable contains the result of multiplyint x and w1\n",
    "y_pred = tf.matmul(h, w2)                                                    # The predicted values of the output\n",
    "diff = y_pred - y                                                            # Calculating the difference between the predicted and the real values of the output\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis = 1))                      # Calculating the loss function by using the mean square error method\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])                              # Calculate the loss of gradinet with respect to w1 and w2\n",
    "\n",
    "learning_rate = 1e-5                                                         # Assigne the learning rate\n",
    "new_w1 = w1.assign(w1 - learning_rate*grad_w1)                               # Update w1\n",
    "new_w2 = w2.assign(w2 - learning_rate*grad_w2)                               # Update w1\n",
    "\n",
    "with tf.Session() as sess:                                                   # Opening a session\n",
    "    sess.run(tf.global_variables_initializer())                              # Initialize the variables\n",
    "    values = {x: np.random.randn(N, D),                                      # Assigning a random values for the placeholders\n",
    "              y: np.random.randn(N, D)}\n",
    "    for t in range(50):                                                      # traning loop with 50 iterations\n",
    "        loss_val, = sess.run([loss], feed_dict = values)                     # Running the computational graph    \n",
    "        print(loss_val)                                                      # Printing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code in the previous cell has a problem that the loss not going down each step, so we will do:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code version 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49072380.0\n",
      "17083582.0\n",
      "8829200.0\n",
      "5111463.0\n",
      "3167654.8\n",
      "2058837.5\n",
      "1384816.5\n",
      "956465.25\n",
      "674228.3\n",
      "483212.97\n",
      "350803.8\n",
      "257489.6\n",
      "190767.16\n",
      "142252.47\n",
      "106909.664\n",
      "80874.54\n",
      "61570.1\n",
      "47175.312\n",
      "36362.14\n",
      "28192.11\n",
      "21986.258\n",
      "17241.379\n",
      "13606.946\n",
      "10816.289\n",
      "8662.755\n",
      "6995.782\n",
      "5702.322\n",
      "4695.1196\n",
      "3908.4612\n",
      "3292.393\n",
      "2808.6128\n",
      "2428.1128\n",
      "2128.1301\n",
      "1891.1384\n",
      "1703.5667\n",
      "1554.8309\n",
      "1436.6846\n",
      "1342.7344\n",
      "1267.9481\n",
      "1208.3622\n",
      "1160.8772\n",
      "1123.0078\n",
      "1092.8115\n",
      "1068.6895\n",
      "1049.4131\n",
      "1034.0033\n",
      "1021.69006\n",
      "1011.82336\n",
      "1003.9459\n",
      "997.6482\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (input)\n",
    "y = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (output)\n",
    "w1 = tf.Variable(tf.random_normal((D, H)))                                   # 1000 x 100 variable   (weights)\n",
    "w2 = tf.Variable(tf.random_normal((H, D)))                                   # 100 x 1000 variable   (weights)\n",
    "\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)                                          # Temporary variable contains the result of multiplyint x and w1\n",
    "y_pred = tf.matmul(h, w2)                                                    # The predicted values of the output\n",
    "diff = y_pred - y                                                            # Calculating the difference between the predicted and the real values of the output\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff**2, axis = 1))                      # Calculating the loss function by using the mean square error method\n",
    "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])                              # Calculate the loss of gradinet with respect to w1 and w2\n",
    "\n",
    "learning_rate = 1e-5                                                         # Assigne the learning rate\n",
    "new_w1 = w1.assign(w1 - learning_rate*grad_w1)                               # Update w1\n",
    "new_w2 = w2.assign(w2 - learning_rate*grad_w2)                               # Update w1\n",
    "updates = tf.group(new_w1, new_w2)                                           # Putting new_w1 and new_w2 into one place\n",
    "\n",
    "with tf.Session() as sess:                                                   # Opening a session\n",
    "    sess.run(tf.global_variables_initializer())                              # Initialize the variables\n",
    "    values = {x: np.random.randn(N, D),                                      # Assigning a random values for the placeholders\n",
    "              y: np.random.randn(N, D)}\n",
    "    losses = []                                                              # Empty place\n",
    "    for t in range(50):                                                      # training loop with 50 iterations\n",
    "        loss_val, _ = sess.run([loss, updates], feed_dict = values)          # run the computational graph\n",
    "        print(loss_val)                                                      # print the loss values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code version 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53387330.0\n",
      "18333184.0\n",
      "9645373.0\n",
      "5710844.0\n",
      "3627635.5\n",
      "2419914.0\n",
      "1672197.2\n",
      "1186658.8\n",
      "859759.0\n",
      "632491.4\n",
      "471683.06\n",
      "355541.06\n",
      "270281.56\n",
      "207202.36\n",
      "159935.81\n",
      "124194.945\n",
      "96922.68\n",
      "76020.52\n",
      "59893.07\n",
      "47387.414\n",
      "37636.203\n",
      "30010.367\n",
      "24024.918\n",
      "19303.77\n",
      "15575.615\n",
      "12621.633\n",
      "10275.998\n",
      "8420.737\n",
      "6946.715\n",
      "5771.796\n",
      "4832.9614\n",
      "4081.2656\n",
      "3478.3884\n",
      "2994.166\n",
      "2604.7249\n",
      "2291.1152\n",
      "2038.2766\n",
      "1834.2617\n",
      "1669.4802\n",
      "1536.2458\n",
      "1428.4622\n",
      "1341.186\n",
      "1270.3828\n",
      "1213.1931\n",
      "1166.895\n",
      "1129.4792\n",
      "1099.2377\n",
      "1074.8423\n",
      "1055.1044\n",
      "1039.1637\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (input)\n",
    "y = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (output)\n",
    "w1 = tf.Variable(tf.random_normal((D, H)))                                   # 1000 x 100 variable   (weights)\n",
    "w2 = tf.Variable(tf.random_normal((H, D)))                                   # 100 x 1000 variable   (weights)\n",
    "\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)                                          # Temporary variable contains the result of multiplyint x and w1\n",
    "y_pred = tf.matmul(h, w2)                                                    # The predicted values of the output\n",
    "diff = y_pred - y                                                            # Calculating the difference between the predicted and the real values of the output\n",
    "loss = tf.reduce_mean(tf.reduce_sum(diff*diff, axis = 1))                    # Calculating the loss function by using the mean square error method\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-5)                          # Use an optimizer to compute gradients\n",
    "updates = optimizer.minimize(loss)                                           # Update weights\n",
    "\n",
    "with tf.Session() as sess:                                                   # Opening a session\n",
    "    sess.run(tf.global_variables_initializer())                              # Initialize the variables\n",
    "    values = {x: np.random.randn(N, D),                                      # Assigning a random values for the placeholders\n",
    "              y: np.random.randn(N, D)}\n",
    "    losses = []                                                              # Empty place\n",
    "    for t in range(50):                                                      # training loop with 50 iterations\n",
    "        loss_val, _ = sess.run([loss, updates], feed_dict = values)          # run the computational graph\n",
    "        print(loss_val)                                                      # print the loss values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code version 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53310.562\n",
      "48207.816\n",
      "43829.438\n",
      "40033.434\n",
      "36713.574\n",
      "33786.848\n",
      "31189.867\n",
      "28871.877\n",
      "26792.12\n",
      "24917.72\n",
      "23221.557\n",
      "21680.787\n",
      "20276.47\n",
      "18992.629\n",
      "17815.58\n",
      "16733.842\n",
      "15737.347\n",
      "14817.33\n",
      "13966.456\n",
      "13177.886\n",
      "12445.686\n",
      "11764.732\n",
      "11130.816\n",
      "10539.682\n",
      "9987.816\n",
      "9471.885\n",
      "8989.078\n",
      "8536.686\n",
      "8112.398\n",
      "7714.098\n",
      "7339.6826\n",
      "6987.518\n",
      "6655.932\n",
      "6343.5454\n",
      "6048.941\n",
      "5770.768\n",
      "5507.9653\n",
      "5259.644\n",
      "5024.808\n",
      "4802.4927\n",
      "4592.002\n",
      "4392.574\n",
      "4203.485\n",
      "4024.1294\n",
      "3853.9\n",
      "3692.2598\n",
      "3538.6858\n",
      "3392.6526\n",
      "3253.7715\n",
      "3121.5854\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (input)\n",
    "y = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (output)\n",
    "w1 = tf.Variable(tf.random_normal((D, H)))                                   # 1000 x 100 variable   (weights)\n",
    "w2 = tf.Variable(tf.random_normal((H, D)))                                   # 100 x 1000 variable   (weights)\n",
    "\n",
    "h = tf.maximum(tf.matmul(x, w1), 0)                                          # Temporary variable contains the result of multiplyint x and w1\n",
    "y_pred = tf.matmul(h, w2)                                                    # The predicted values of the output\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)                               # use predefined common losses\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-3)                          # Use an optimizer to compute gradients\n",
    "updates = optimizer.minimize(loss)                                           # Update weights\n",
    "\n",
    "with tf.Session() as sess:                                                   # Opening a session\n",
    "    sess.run(tf.global_variables_initializer())                              # Initialize the variables\n",
    "    values = {x: np.random.randn(N, D),                                      # Assigning a random values for the placeholders\n",
    "              y: np.random.randn(N, D)}\n",
    "    losses = []                                                              # Empty place\n",
    "    for t in range(50):                                                      # training loop with 50 iterations\n",
    "        loss_val, _ = sess.run([loss, updates], feed_dict = values)          # run the computational graph\n",
    "        print(loss_val)                                                      # print the loss values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code version 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9078853\n",
      "1.214218\n",
      "1.0278909\n",
      "0.9896201\n",
      "0.98079515\n",
      "0.97844297\n",
      "0.9776362\n",
      "0.97729117\n",
      "0.97706896\n",
      "0.97688353\n",
      "0.9767067\n",
      "0.9765339\n",
      "0.9763623\n",
      "0.9761911\n",
      "0.97602004\n",
      "0.9758491\n",
      "0.9756782\n",
      "0.9755073\n",
      "0.97533643\n",
      "0.97516555\n",
      "0.9749946\n",
      "0.9748236\n",
      "0.9746528\n",
      "0.9744818\n",
      "0.9743109\n",
      "0.97413987\n",
      "0.9739689\n",
      "0.973798\n",
      "0.973627\n",
      "0.97345597\n",
      "0.9732848\n",
      "0.97311383\n",
      "0.97294277\n",
      "0.9727716\n",
      "0.97260046\n",
      "0.9724292\n",
      "0.9722581\n",
      "0.9720869\n",
      "0.97191554\n",
      "0.97174424\n",
      "0.971573\n",
      "0.9714016\n",
      "0.9712301\n",
      "0.9710587\n",
      "0.9708872\n",
      "0.97071576\n",
      "0.9705441\n",
      "0.97037244\n",
      "0.9702008\n",
      "0.9700292\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (input)\n",
    "y = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (output)\n",
    "\n",
    "init = tf.variance_scaling_initializer(2.0)                                  # use the intitializer\n",
    "h = tf.layers.dense(inputs = x, units = H, activation = tf.nn.relu, kernel_initializer = init)     # Define the input layer\n",
    "y_pred = tf.layers.dense(inputs = h, units = D, kernel_initializer = init)                        # Define the output layer\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)                               # use predefined common losses\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e0)                           # Use an optimizer to compute gradients\n",
    "updates = optimizer.minimize(loss)                                           # Update weights\n",
    "\n",
    "with tf.Session() as sess:                                                   # Opening a session\n",
    "    sess.run(tf.global_variables_initializer())                              # Initialize the variables\n",
    "    values = {x: np.random.randn(N, D),                                      # Assigning a random values for the placeholders\n",
    "              y: np.random.randn(N, D)}\n",
    "    for t in range(50):                                                      # training loop with 50 iterations\n",
    "        loss_val, _ = sess.run([loss, updates], feed_dict = values)          # run the computational graph\n",
    "        print(loss_val)                                                      # print the loss values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code version 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1685723\n",
      "1.1308577\n",
      "1.1016709\n",
      "1.0783638\n",
      "1.0593646\n",
      "1.0436316\n",
      "1.0304482\n",
      "1.0193079\n",
      "1.0097802\n",
      "1.0015754\n",
      "0.99442506\n",
      "0.9881281\n",
      "0.9825398\n",
      "0.97754323\n",
      "0.9730388\n",
      "0.9689579\n",
      "0.9652262\n",
      "0.96178466\n",
      "0.958587\n",
      "0.95558083\n",
      "0.95274866\n",
      "0.9500563\n",
      "0.9474765\n",
      "0.94498223\n",
      "0.9425532\n",
      "0.94018227\n",
      "0.9378467\n",
      "0.93554235\n",
      "0.9332591\n",
      "0.93098825\n",
      "0.9287213\n",
      "0.9264533\n",
      "0.92417836\n",
      "0.9218898\n",
      "0.9195886\n",
      "0.91726583\n",
      "0.9149236\n",
      "0.9125605\n",
      "0.9101723\n",
      "0.90775406\n",
      "0.9053065\n",
      "0.902829\n",
      "0.90031886\n",
      "0.8977759\n",
      "0.89519787\n",
      "0.89258456\n",
      "0.8899349\n",
      "0.88724595\n",
      "0.88451576\n",
      "0.8817465\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 64, 1000, 100\n",
    "x = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (input)\n",
    "y = tf.placeholder(tf.float32, shape = (N, D))                               # 64 x 1000 placeholder   (output)\n",
    "\n",
    "model = tf.keras.Sequential()                                                # Build the neural network model\n",
    "model.add(tf.keras.layers.Dense(H, input_shape = (D, ), activation = tf.nn.relu))     # Add a layer to the model\n",
    "model.add(tf.keras.layers.Dense(D))                                                   # Add a layer to the model\n",
    "y_pred = model(x)                                                            # Calculate thr predicted output (get output by calling the model), note no computation only building the computational graph\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)                               # use predefined common losses\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e0)                           # Use an optimizer to compute gradients\n",
    "updates = optimizer.minimize(loss)                                           # Update weights\n",
    "\n",
    "with tf.Session() as sess:                                                   # open session\n",
    "    sess.run(tf.global_variables_initializer())                              # Initialize the variales\n",
    "    values = {x: np.random.randn(N, D),                                      # Assigning a random values for the placeholders\n",
    "              y: np.random.randn(N, D)}\n",
    "    for t in range(50):                                                      # Training loop with 50 iterations\n",
    "        loss_val, _ = sess.run([loss, updates], feed_dict = values)          # Calculate the losses\n",
    "        print(loss_val)                                                      # Print the losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code version 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "64/64 [==============================] - 1s 11ms/sample - loss: 1.1595\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - 0s 244us/sample - loss: 1.1220\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - 0s 0s/sample - loss: 1.0927\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - 0s 244us/sample - loss: 1.0692\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - 0s 244us/sample - loss: 1.0500\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - 0s 78us/sample - loss: 1.0341\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - 0s 156us/sample - loss: 1.0207\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - 0s 94us/sample - loss: 1.0094\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - 0s 188us/sample - loss: 0.9997\n",
      "Epoch 10/50\n",
      "64/64 [==============================] - 0s 172us/sample - loss: 0.9913\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - 0s 78us/sample - loss: 0.9840\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - 0s 156us/sample - loss: 0.9775\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - 0s 125us/sample - loss: 0.9718\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - 0s 234us/sample - loss: 0.9667\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - 0s 156us/sample - loss: 0.9621\n",
      "Epoch 16/50\n",
      "64/64 [==============================] - 0s 141us/sample - loss: 0.9579\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - 0s 78us/sample - loss: 0.9541\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - 0s 109us/sample - loss: 0.9506\n",
      "Epoch 19/50\n",
      "64/64 [==============================] - 0s 94us/sample - loss: 0.9474\n",
      "Epoch 20/50\n",
      "64/64 [==============================] - 0s 125us/sample - loss: 0.9443\n",
      "Epoch 21/50\n",
      "64/64 [==============================] - 0s 188us/sample - loss: 0.9414\n",
      "Epoch 22/50\n",
      "64/64 [==============================] - 0s 109us/sample - loss: 0.9387\n",
      "Epoch 23/50\n",
      "64/64 [==============================] - 0s 125us/sample - loss: 0.9361\n",
      "Epoch 24/50\n",
      "64/64 [==============================] - 0s 125us/sample - loss: 0.9335\n",
      "Epoch 25/50\n",
      "64/64 [==============================] - 0s 109us/sample - loss: 0.9311\n",
      "Epoch 26/50\n",
      "64/64 [==============================] - 0s 141us/sample - loss: 0.9287\n",
      "Epoch 27/50\n",
      "64/64 [==============================] - 0s 219us/sample - loss: 0.9263\n",
      "Epoch 28/50\n",
      "64/64 [==============================] - 0s 78us/sample - loss: 0.9240\n",
      "Epoch 29/50\n",
      "64/64 [==============================] - 0s 125us/sample - loss: 0.9217\n",
      "Epoch 30/50\n",
      "64/64 [==============================] - 0s 78us/sample - loss: 0.9194\n",
      "Epoch 31/50\n",
      "64/64 [==============================] - 0s 94us/sample - loss: 0.9171\n",
      "Epoch 32/50\n",
      "64/64 [==============================] - 0s 141us/sample - loss: 0.9148\n",
      "Epoch 33/50\n",
      "64/64 [==============================] - 0s 78us/sample - loss: 0.9126\n",
      "Epoch 34/50\n",
      "64/64 [==============================] - 0s 78us/sample - loss: 0.9103\n",
      "Epoch 35/50\n",
      "64/64 [==============================] - 0s 125us/sample - loss: 0.9080\n",
      "Epoch 36/50\n",
      "64/64 [==============================] - 0s 156us/sample - loss: 0.9057\n",
      "Epoch 37/50\n",
      "64/64 [==============================] - 0s 141us/sample - loss: 0.9034\n",
      "Epoch 38/50\n",
      "64/64 [==============================] - 0s 141us/sample - loss: 0.9010\n",
      "Epoch 39/50\n",
      "64/64 [==============================] - 0s 125us/sample - loss: 0.8986\n",
      "Epoch 40/50\n",
      "64/64 [==============================] - 0s 156us/sample - loss: 0.8962\n",
      "Epoch 41/50\n",
      "64/64 [==============================] - 0s 78us/sample - loss: 0.8938\n",
      "Epoch 42/50\n",
      "64/64 [==============================] - 0s 141us/sample - loss: 0.8914\n",
      "Epoch 43/50\n",
      "64/64 [==============================] - 0s 109us/sample - loss: 0.8889\n",
      "Epoch 44/50\n",
      "64/64 [==============================] - 0s 141us/sample - loss: 0.8864\n",
      "Epoch 45/50\n",
      "64/64 [==============================] - 0s 172us/sample - loss: 0.8838\n",
      "Epoch 46/50\n",
      "64/64 [==============================] - 0s 109us/sample - loss: 0.8813\n",
      "Epoch 47/50\n",
      "64/64 [==============================] - 0s 141us/sample - loss: 0.8787\n",
      "Epoch 48/50\n",
      "64/64 [==============================] - 0s 109us/sample - loss: 0.8760\n",
      "Epoch 49/50\n",
      "64/64 [==============================] - 0s 94us/sample - loss: 0.8733\n",
      "Epoch 50/50\n",
      "64/64 [==============================] - 0s 109us/sample - loss: 0.8706\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 64, 1000, 100\n",
    "model = tf.keras.Sequential()                                                         # Build the neural network model\n",
    "model.add(tf.keras.layers.Dense(H, input_shape = (D, ), activation = tf.nn.relu))     # Add a layer to the model\n",
    "model.add(tf.keras.layers.Dense(D))                                                   # Add a layer to the model\n",
    "\n",
    "model.compile(loss = tf.keras.losses.mean_squared_error, optimizer = tf.keras.optimizers.SGD(lr = 1e0))     # Define the loss and the optimizer\n",
    "\n",
    "x = np.random.randn(N, D)                                                             # Assigning a random values for x\n",
    "y = np.random.randn(N, D)                                                             # Assigning a random values for y\n",
    "\n",
    "history = model.fit(x, y, epochs = 50, batch_size = N)                                # Training loop with 50 iterations, and batch size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
